import os, io, re
import pandas as pd
import logging
import unicodedata
from PyQt5.QtCore import Qt
from openpyxl import load_workbook
from openpyxl.styles.colors import Color
from collections import defaultdict


# Allowed part codes from PDF filenames
ALLOWED_PART_CODES = ['UU1_DOM', 'DOM', 'UU1', '2LB', '2XV', '4LB', '19L', '19A', '21A', 'DC1']

def normalize_headers(df):
    rename = {}
    for col in df.columns:
        name = str(col).strip().lower()
        # Requirement
        if ("require" in name) or ("‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î" in name) or ("‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠" in name):
            rename[col] = "Requirement"
            continue
        # Term
        if ("symbol" in name) or ("exact" in name) or ("term" in name):
            rename[col] = "Symbol/ Exact wording"
            continue
        # Spec
        if ("spec" in name) or ("specification" in name):
            rename[col] = "Specification"
            continue

        # Package Panel
        if ("package panel" in name) or ("package" in name and "panel" in name):
            rename[col] = "Package Panel"
            continue

        # Procedure
        if ("procedure" in name) or ("process" in name):
            rename[col] = "Procedure"
            continue

        # Remark
        if ("remark" in name) or ("‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏" in name):
            rename[col] = "Remark"
            continue

    df = df.rename(columns=rename)

    if "Requirement" not in df.columns:
        raise ValueError("Checklist Excel must contain a column recognizable as 'Requirement'.")
    return df

def get_strikeout_or_red_text_rows(excel_path, sheet_name, header_row_index):
    wb = load_workbook(excel_path)
    ws = wb[sheet_name]
    bad_rows = set()

    for row in ws.iter_rows(min_row=header_row_index + 2):  
        row_index = row[0].row
        for cell in row:
            font = cell.font
            color = font.color

            if not font:
                continue

            is_strike = font.strike
            is_red = False

            if color:
                if color.type == "rgb" and color.rgb:
                    is_red = color.rgb.upper().startswith("FF0000")
                elif color.type == "theme" and hasattr(color, "theme"):
                    if color.theme == 10:
                        is_red = True

            if is_strike and is_red:
                bad_rows.add(row_index)
                break 
    return bad_rows

def extract_part_code_from_pdf(pdf_filename):
    basename = os.path.basename(pdf_filename).upper().replace(" ", "").replace(",", "")
    detected = [code for code in ALLOWED_PART_CODES if code in basename]
    return list(set(detected))

def fuzzy_find_columns(df):
    term_col = None
    lang_col = None
    spec_col = None  

    logging.info(f"üßæ Columns found in sheet: {list(df.columns)}")

    # add Fallback mapping ‡∏ñ‡πâ‡∏≤ fuzzy ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
    def _norm(s: object) -> str:
         return str(s).replace("\xa0", " ").strip().lower()

    # ‡∏ï‡∏£‡∏á Keyword ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô 
    for col in df.columns:
        if pd.isna(col): 
            continue
        n = _norm(col)
        n_compact = n.replace(" ", "")

        if term_col is None and any(k in n_compact for k in ["term", "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°", "exactwording", "symbol", "wording"]):
            term_col = col

        if lang_col is None and any(k in n_compact for k in ["languagecode", "langcode", "language", "lang", "‡∏†‡∏≤‡∏©‡∏≤"]):
            lang_col = col

        if spec_col is None and any(k in n_compact for k in ["specification", "spec", "requirement", "‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î"]):
            spec_col = col

    # Fallback ‡∏ú‡πà‡∏≠‡∏ô‡πÄ‡∏á‡∏∑‡πà‡∏≠‡πÑ‡∏Ç
    if term_col is None:
        for col in df.columns:
            n = _norm(col)
            n_compact = n.replace(" ", "")
            if any(k in n_compact for k in ["term", "symbol", "exact", "wording"]):
                term_col = col
                break

    if lang_col is None:
        for col in df.columns:
            n = _norm(col)
            if ("language" in n) or (n == "lang") or ("language code" in n) or ("lang code" in n):
                lang_col = col
                break

    if spec_col is None:
        for col in df.columns:
            n = _norm(col)
            if (n == "spec") or ("specification" in n) or ("requirement" in n) or ("‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î" in n):
                spec_col = col
                break

    # OPTIONAL ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Requirement ‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
    for c in df.columns:
        if str(c).strip().lower() == "requirement":
            if c != "Requirement":
                df = df.rename(columns={c: "Requirement"})
            break

    logging.info(f"üõü Fallback columns ‚Üí Term: {term_col}, Language: {lang_col}, Spec: {spec_col}")
    return term_col, lang_col, spec_col

def normalize_text(text):
    if not isinstance(text, str):
        return ""
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r"\([^)]*\)", "", text) 
    text = text.replace("‚Äô", "'")  
    text = re.sub(r"\s+", " ", text)    
    return text.strip().lower()

def load_checklist(excel_path, pdf_filename=None):
    all_sheets = pd.read_excel(excel_path, sheet_name=None)
    sheet_names = list(all_sheets.keys())

    if not pdf_filename:
        raise ValueError("üìÑ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö Sheet ‡∏Ç‡∏≠‡∏á Checklist")

    part_codes = extract_part_code_from_pdf(pdf_filename)
    if not part_codes:
        raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Part code ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå PDF")

    logging.info(f"üìÇ PDF filename: {pdf_filename}")
    logging.info(f"üß† Part codes detected: {part_codes}")
    logging.info(f"üìÑ Sheet names: {sheet_names}")

    for sheet_name in sheet_names:
        sheet_name_normalized = sheet_name.upper().replace(" ", "")
        for code in part_codes:
            if sheet_name_normalized.startswith(code):
                logging.info(f"‚úÖ Found matching sheet: {sheet_name}")
                df = all_sheets[sheet_name]

                HEADER_HINTS = [
                    r"\brequirement\b",
                    r"\blanguage\b|\blang\b|\blanguage\s*code\b",
                    r"\bsymbol\b|\bexact\s*wording\b|\bterm\b",
                    r"\bspec(ification)?\b",
                ]

                def _row_score(cells):
                    score = 0
                    for c in cells:
                        s = "" if c is None else str(c).strip()
                        s_norm = s.lower()
                        if not s:
                            continue

                        # ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πâ‡∏°‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                        if any(re.search(p, s_norm) for p in HEADER_HINTS):
                            score += 5
                        # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏¢‡∏≤‡∏ß/‡∏°‡∏µ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå = ‡πÄ‡∏¢‡∏≠‡∏∞
                        if len(s) <= 24:
                            score += 1
                        if "=" in s or "‚Äú" in s or "‚Äù" in s:
                            score -= 2
                    return score
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ header ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 15 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î
                header_row_index = None
                best_score = -10
                scan_upto = min(15, len(df))
                for i in range(scan_upto):
                    row_vals = list(df.iloc[i].values)
                    if pd.Series(row_vals).notna().sum() < 2:
                        continue
                    sc = _row_score(row_vals)
                    if sc > best_score:
                        best_score = sc
                        header_row_index = i

                if header_row_index is None:
                    raise ValueError(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÅ‡∏ñ‡∏ß header ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏ô sheet: {sheet_name}")
                
                df.columns = df.iloc[header_row_index]
                df = df[header_row_index + 1:].reset_index(drop=True)
                logging.info(f"üßæ Header chosen at row {header_row_index+1} | columns: {list(df.columns)[:6]}...")

                # Column Mapping
                term_col, lang_col, spec_col = fuzzy_find_columns(df)
                logging.info(f"üîé ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Term: {term_col}, Language: {lang_col}, Spec: {spec_col}")

                # Standardize
                if spec_col and spec_col in df.columns:
                    df[spec_col] = df[spec_col].apply(
                        lambda x: "-" if pd.isna(x) or str(x).strip().upper() in ["N/A", "NONE", "-"] else str(x)
                    )

                # ffill ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
                columns_to_ffill = [c for c in df.columns if str(c).strip().lower() in ["requirement", "language"]]
                if columns_to_ffill:
                    df[columns_to_ffill] = df[columns_to_ffill].ffill()

                # Rename ‡∏ñ‡πâ‡∏≤ term_col ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô KeyError
                if term_col in df.columns:
                    df = df.rename(columns={term_col: "Symbol/Exact wording"})
                elif "Symbol/Exact wording" not in df.columns:
                    df["Symbol/Exact wording"] = "-"

                GROUP_RE = re.compile(r"^\s*\[GROUP:\s*(?P<name>.+?)\s*\]\s*\[(?P<mode>ANY|ALL)\]\s*$", re.IGNORECASE)

                def _split_simple_list(cell: str):
                    """‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏≤‡∏ò‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ; | ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà"""
                    if not isinstance(cell, str):
                        return []
                    s = cell.strip()
                    if not s or s in ["-", "N/A", "None"]:
                        return []
                    parts = re.split(r"[;\n|]", s.replace("\r", ""))
                    return [p.strip().replace("\\", "/") for p in parts if p.strip()]
                
                def _parse_image_groups(cell: str):
                    """
                    ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö:
                    - ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ group/tag:
                        [GROUP: Old logo][ALL]
                        //server/share/old1.png
                        //server/share/old2.png
                        [GROUP: New logo][ANY]
                        assets/new1.png
                        assets/new2.png
                    - ‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤: ‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏≤‡∏ò‡πÉ‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß -> ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß mode=ANY
                    - ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Image Match ‡πÅ‡∏¢‡∏Å: ‡∏à‡∏∞‡πÑ‡∏õ normalize ‡∏ï‡πà‡∏≠
                    """
                    if not isinstance(cell, str) or not cell.strip():
                        return []
                    lines = [ln.strip() for ln in cell.replace("\r", "").split("\n")]
                    groups, cur = [], None
                    for ln in lines:
                        if not ln:
                            continue
                        m = GROUP_RE.match(ln)
                        if m:
                            if cur and cur["paths"]:
                                groups.append(cur)
                            cur = {"name": m.group("name"), "mode": m.group("mode").lower(), "paths": []}
                        else:
                            for p in re.split(r"[;|]", ln):
                                p = p.strip()
                                if p and p not in ["-", "N/A", "None"]:
                                    if cur is None:
                                        cur = {"name": "", "mode": "any", "paths": []}
                                    cur["paths"].append(p.replace("\\", "/"))
                    if cur and cur["paths"]:
                        groups.append(cur)

                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ [GROUP:...] ‡πÄ‡∏•‡∏¢ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏û‡∏≤‡∏ò‡πÄ‡∏î‡∏µ‡∏¢‡∏ß/‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏≤‡∏ò -> ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ANY 
                    if not groups:
                        paths = _split_simple_list(cell)
                        return [{"name": "", "mode": "any", "paths": paths}] if paths else []
                    return groups
                
                def _flatten_paths(groups):
                    out = []
                    for g in (groups or []):
                        out.extend(g.get("paths", []))
                    return out 
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Image_Groups + Image_Paths_Flat
                if "Image Path" in df.columns:
                    df["Image_Groups"] = df["Image Path"].fillna("").astype(str).apply(_parse_image_groups)
                    df["Image_Paths_Flat"] = df["Image_Groups"].apply(_flatten_paths)
                else:
                    df["Image_Groups"] = [[] for _ in range(len(df))]
                    df["Image_Paths_Flat"] = [[] for _ in range(len(df))]

                # ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Image Match ‡πÅ‡∏¢‡∏Å (ANY/ALL) ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ
                if "Image Match" in df.columns:
                    def _apply_global_mode(groups, mode_cell):
                        mode = (str(mode_cell).strip().lower() if isinstance(mode_cell, str) else "")
                        if mode in ["all", "any"]:
                            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Å‡∏•‡∏∏‡πà‡∏° ‡∏à‡∏∞ apply ‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°
                            for g in groups or []:
                                g["mode"] = mode
                        return groups
                    df["Image_Groups"] = [
                        _apply_global_mode(g, m) for g, m in zip(df["Image_Groups"], df["Image Match"])
                    ] 

                # Add extract hyperlink targets from Remark cells
                try:
                    wb = load_workbook(excel_path, data_only=True)
                    ws = wb[sheet_name]
                    # ‡∏´‡∏≤ index ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Remark ‡∏à‡∏≤‡∏Å df.columns
                    if "Remark" in df.columns:
                        # ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï: +1 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ openpyxl ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà 1
                        remark_col_idx = list(df.columns).index("Remark") + 1
                        start_row = header_row_index + 2  # ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï

                        remark_links = []
                        for i in range(len(df)):
                            cell = ws.cell(row=start_row + i, column=remark_col_idx)
                            url = ""
                            if cell and cell.hyperlink:
                                # openpyxl ‡πÉ‡∏´‡πâ .hyperlink.target ‡πÄ‡∏õ‡πá‡∏ô URL ‡∏à‡∏£‡∏¥‡∏á
                                url = cell.hyperlink.target or ""
                            remark_links.append(url)
                        df["Remark Link"] = remark_links
                    else:
                        df["Remark Link"] = ""
                except Exception as _e:
                    logging.debug(f"Remark hyperlink extraction skipped: {_e}")
                    df["Remark Link"] = ""

                # --- RESOLVE absolute paths for Image_Groups and add _HasImage BEFORE filtering ---

                excel_dir = os.path.dirname(excel_path)

                def _resolve_group_paths(groups):
                    out = []
                    for g in (groups or []):
                        paths = []
                        for p in g.get("paths", []):
                            if not isinstance(p, str) or not p.strip():
                                continue
                            p2 = p.strip().replace("\\", os.sep).replace("/", os.sep)
                            if not os.path.isabs(p2):
                                p2 = os.path.abspath(os.path.join(excel_dir, p2))
                            paths.append(p2)
                        out.append({"name": g.get("name",""), "mode": (g.get("mode") or "any").lower(), "paths": paths})
                    return out

                df["Image_Groups_Resolved"] = df["Image_Groups"].apply(_resolve_group_paths)

                def _has_any_image(groups):
                    if not groups:
                        return False
                    for g in groups:
                        for p in g.get("paths", []):
                            if isinstance(p, str) and p.strip():
                                return True
                    return False

                df["_HasImage"] = df["Image_Groups_Resolved"].apply(_has_any_image)

                # (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Image Path ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß ‡∏Å‡πá resolve ‡πÉ‡∏´‡πâ‡∏î‡πâ‡∏ß‡∏¢)
                if "Image Path" in df.columns:
                    df["Image Path"] = df["Image Path"].fillna("").astype(str)
                else:
                    df["Image Path"] = ""

                def _resolve_path(p):
                    if not isinstance(p, str) or not p.strip():
                        return ""
                    p = p.strip().replace("\\", os.sep).replace("/", os.sep)
                    if os.path.isabs(p):
                        return p
                    return os.path.abspath(os.path.join(excel_dir, p))

                df["Image Path Resolved"] = df["Image Path"].apply(_resolve_path)
                
                # ‡∏ï‡∏±‡∏î‡πÅ‡∏ñ‡∏ß‡∏ß‡πà‡∏≤‡∏á/‡πÅ‡∏ñ‡∏ß‡∏ú‡∏µ‡∏´‡∏•‡∏±‡∏á
                def _clean(s):
                    return str(s).strip().lower()

                # ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏ö‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏°‡∏µ spec_col)
                term_col_safe = "Symbol/Exact wording"
                spec_col_safe = spec_col if spec_col in df.columns else None

                # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ß‡πà‡∏≤‡∏á
                term_empty  = df[term_col_safe].astype(str).str.strip().isin(["", "-", "nan", "none", "n/a"])
                if spec_col_safe:
                    spec_empty  = df[spec_col_safe].astype(str).str.strip().isin(["", "-", "nan", "none", "n/a"])
                else:
                    spec_empty  = pd.Series([True] * len(df), index=df.index) # Series ‡∏Ç‡∏≠‡∏á True ‡∏¢‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤ df (‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ô‡∏µ‡πâ‡πÑ‡∏õ)

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Series ‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Remark ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                remark_series = df["Remark"] if "Remark" in df.columns else pd.Series([""] * len(df), index=df.index)
                remark_empty = remark_series.astype(str).str.strip().isin(["", "-", "nan"])

                # Add ‡∏•‡∏¥‡∏á‡∏Å‡πå Remark ‡∏Å‡πá‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ "‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
                remark_link_series = df.get("Remark Link", pd.Series([""] * len(df), index=df.index))
                remark_link_empty = remark_link_series.astype(str).str.strip().isin(["", "-", "nan", ""])

                # ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö = ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Term ‡∏´‡∏£‡∏∑‡∏≠ Spec ‡∏´‡∏£‡∏∑‡∏≠ Remark ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á
                keep_mask = ~(term_empty & spec_empty & remark_empty & remark_link_empty) | df["_HasImage"]
                df = df[keep_mask].reset_index(drop=True)

                # ‡∏Å‡∏±‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏•‡πâ‡∏ß‡∏ô (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô Requirement/Language) ‡πÄ‡∏õ‡πá‡∏ô NaN ‡∏•‡πâ‡∏ß‡∏ô
                non_struct_cols = [c for c in df.columns if str(c).strip().lower() not in ["requirement", "language"]]
                df = df[~df[non_struct_cols].isna().all(axis=1)].reset_index(drop=True)

                # Drop Red+Strike Rows (‡∏Ñ‡∏á log ‡πÑ‡∏ß‡πâ ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô debug)
                bad_row_numbers = get_strikeout_or_red_text_rows(excel_path, sheet_name, header_row_index)
                logging.debug(f"‚ùå Red+Strike rows from Excel: {bad_row_numbers}")

                df["ExcelRow"] = df.index + header_row_index + 2 
                df.drop(columns=["ExcelRow"], inplace=True)

                # Drop columns ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
                columns_to_exclude = ["Instruction of Play function feature", "Warning statement"]
                df = df[[col for col in df.columns if col and str(col).strip().lower() not in columns_to_exclude]]

                # Explode multi-term 
                df["Symbol/Exact wording"] = df["Symbol/Exact wording"].astype(str).str.replace("\r", "")
                df["Symbol/Exact wording"] = df["Symbol/Exact wording"].apply(
                    lambda s: [p.strip() for p in s.split("\n") if p.strip()] if ("\n" in s) else [s.strip()]
                )
                df = df.explode("Symbol/Exact wording", ignore_index=True)

                # Language List 
                if lang_col:
                    df = df.rename(columns={lang_col: "Language"})
                    df["Language List"] = df["Language"].apply(lambda x: str(x).split(",") if pd.notna(x) else [])
                else:
                    df["Language List"] = [[] for _ in range(len(df))]

                # Extract from Remark 
                if "Remark" in df.columns:
                    def extract_languages_from_remark(remark, term):
                        langs = []
                        if pd.isna(remark): return langs
                        for line in str(remark).splitlines():
                            if "=" in line:
                                left, right = line.split("=", 1)
                                if term.strip().lower() in left.strip().lower():
                                    langs.append(right.strip())
                        return langs
                    df["Language List"] = [
                        extract_languages_from_remark(remark, term) or ["Unspecified"]
                        for remark, term in zip(df.get("Remark", []), df["Symbol/Exact wording"])
                    ]

                return df 

    raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Sheet ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Part code ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå PDF")

def start_check(df_checklist, extracted_text_list):

    logger = logging.getLogger(__name__)

    results = []
    all_texts = []
    grouped = defaultdict(list)

    skip_keywords = [
        "do not print", "do not forget", "see template", "don't forget",
        "for reference", "note", "click", "reminder", "refer template", "remove from template", "remove from mb legal template"
    ]

    manual_keywords = [
        "brand logo", "copyright for t&f", "space for date code", "lion mark", "lionmark", "lion-mark", "ce mark", "en 71",
        "ukca", "mc mark", "cib", "argentina logo", "brazilian logo", "italy requirement", "france requirement",
        "sorting & donation label", "spain sorting symbols", "usa warning statement", "international warning statement",
        "upc requirement", "list of content : text", "list of content : pictorial", "product‚Äôs common", "product's common", "generic name"
    ]

    def is_artwork_page(page_items):
        if len(page_items) < 10:
            return False
        for item in page_items:
            if isinstance(item, dict) and float(item.get("size", 0)) >= 1.6:
                return True
        return False

    artwork_pages = []
    page_mapping = {}

    for real_page_index, page_items in enumerate(extracted_text_list):
        if is_artwork_page(page_items):
            artwork_pages.append(page_items)
            page_mapping[len(artwork_pages)] = real_page_index + 1

    logger.info(
    "üìÑ Artwork-like pages considered: %d (real pages: %s)",
    len(artwork_pages),
    list(page_mapping.values())
    )

    for artwork_index, page_items in enumerate(artwork_pages):
        for item in page_items:
            text_norm = normalize_text(item.get("text", ""))
            page_number = page_mapping[artwork_index + 1]
            all_texts.append((text_norm, page_number, item))

    for idx, row in df_checklist.iterrows():
        requirement = str(row.get("Requirement", "")).strip()
        spec = str(row.get("Specification", "")).strip()
        package_panel = (str(row.get("Package Panel", "")) or "").strip() or "-"
        procedure = (str(row.get("Procedure", "")) or "").strip() or "-"
        remark_text = (str(row.get("Remark", "")) or "").strip()
        remark_link = (str(row.get("Remark Link", "")) or "").strip()

        # Normalize
        req_norm = normalize_text(requirement)
        spec = "-" if spec.lower() in ["", "n/a", "none", "unspecified", "nan"] else spec
        spec_norm = normalize_text(spec)

        # Term (‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÅ‡∏ñ‡∏ß‡∏≠‡∏∑‡πà‡∏ô)
        term_raw = row.get("Symbol/Exact wording", None)
        term_cell_raw = str(term_raw) if pd.notna(term_raw) else ""
        term_cell_clean = term_cell_raw.strip()
        term_cell_clean = "-" if term_cell_clean.lower() in ["", "n/a", "none", "unspecified", "nan"] else term_cell_clean
        term_lines = [term_cell_clean] if term_cell_clean != "-" else []

        # ‡∏ï‡∏£‡∏ß‡∏à Manual
        is_manual = any(kw in req_norm for kw in manual_keywords)

        # ‡∏Ç‡πâ‡∏≤‡∏° row ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        if any(kw in req_norm for kw in skip_keywords) or any(kw in spec_norm for kw in skip_keywords):
            continue

        # VERIFIED SECTION
        if not is_manual:
            if not term_lines:
                has_image = bool(row.get("_HasImage", False))
                if has_image:
                    grouped[(requirement, spec, "Manual")].append({
                        "Term": "-",
                        "Remark": remark_text or "-",
                        "Remark URL": remark_link or "-",
                        "Found": "‚ùå Not Found",
                        "Match": "‚ùå",
                        "Pages": "-",
                        "Font Size": "-",
                        "Note": "Image mapping only (no text term)",
                        "Package Panel": package_panel,
                        "Procedure": procedure,
                    })
                    continue 
                continue

        # --- MANUAL SECTION ---
        if is_manual:
            remark_str = str(row.get("Remark", "")).strip().lower()
            is_term_empty   = (len(term_lines) == 0)
            is_spec_empty   = (str(spec)).strip().lower() in ["", "-", "nan", "none", "null"]
            is_remark_empty = (remark_str in ["", "-", "nan"])

            if is_term_empty and is_spec_empty and is_remark_empty:
                if bool(row.get("_HasImage", False)):
                    grouped[(requirement, spec, "Manual")].append({
                        "Term": "-",
                        "Remark": remark_text or "-",
                        "Remark URL": remark_link or "-",
                        "Found": "‚ùå Not Found",
                        "Match": "‚ùå",
                        "Pages": "-",
                        "Font Size": "-",
                        "Note": "Image mapping only (manual; empty text fields)",
                        "Package Panel": package_panel,
                        "Procedure": procedure,
                    })
                    continue
                else:
                    continue

            logger.info(
                "üü® [MANUAL] Req: '%s' | Spec: '%s' ‚Üí Manual verification",
            requirement, (spec or "-")
            )

            if not term_lines:
                grouped[(requirement, spec, "Manual")].append({
                    "Term": term_cell_raw,
                    "Found": "-",
                    "Match": "-",
                    "Pages": "-",
                    "Font Size": "-",
                    "Note": "Manual check required",
                    "Verification": "Manual",
                    "Remark": remark_text,
                    "Remark URL": remark_link,
                    "Package Panel": package_panel,
                    "Procedure": procedure,
                })
            else:
                for term in term_lines:
                    grouped[(requirement, spec, "Manual")].append({
                    "Term": term,
                    "Found": "-",
                    "Match": "-",
                    "Pages": "-",
                    "Font Size": "-",
                    "Note": "Manual check required",
                    "Verification": "Manual",
                    "Remark": remark_text,
                    "Remark URL": remark_link,
                    "Package Panel": package_panel,
                    "Procedure": procedure,
                    })
            continue

        # VERIFIED SECTION
        for term in term_lines:
            term_norm = normalize_text(term)
            if not term_norm or any(kw in term_norm for kw in skip_keywords):
                continue

            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "or" ‡πÉ‡∏ô term ‡πÄ‡∏ä‡πà‡∏ô "DOM or UU1"
            sep = " or "
            term_variants = [t.strip() for t in term_norm.split(sep)] if sep in term_norm else [term_norm]

            found_pages = []
            matched_items = []
            seen = set () # ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ (page_number, id(item))
            
            # OR loop
            for variant in term_variants:
                if not variant:
                    continue
                for text_norm, page_number, item in all_texts:
                    if variant in text_norm:
                        key = (page_number, id(item))
                        if key not in seen:
                            seen.add(key)
                            found_pages.append(page_number)
                            matched_items.append(item)

            if not found_pages:
                term_words = term_norm.split()
                for text_norm, page_number, item in all_texts:
                    if any(w in text_norm for w in term_words):
                        key = (page_number, id(item))
                        if key not in seen:
                            seen.add(key)
                            found_pages.append(page_number)
                            matched_items.append(item)

            found_flag = "‚úÖ Found" if found_pages else "‚ùå Not Found"
            match_result = "‚úî"
            notes = []

            # ‡∏Å‡∏±‡∏ô list ‡∏ß‡πà‡∏≤‡∏á/‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏ô‡∏¥‡∏î
            spec_lower = spec.lower() if isinstance(spec, str) else "-"
            sizes = [float(i.get("size", 0) or 0) for i in matched_items]
            max_size = max(sizes) if sizes else 0.0
            bolds = [bool(i.get("bold", False)) for i in matched_items]
            underlines = [bool(i.get("underline", False)) for i in matched_items]
            texts = [i.get("text", "") for i in matched_items]

            font_size_str = "-"

            if found_pages and matched_items and spec != "-":
                if "bold" in spec_lower and not any(bolds):
                    match_result = "‚ùå"
                    notes.append("Not Bold")

                if "no underline" in spec_lower:
                    if any(underlines):
                        match_result = "‚ùå"
                        notes.append("Underline must be absent")
                elif "underline" in spec_lower:
                    if not any(underlines):
                        match_result = "‚ùå"
                        notes.append("Underline Missing")

                if "all caps" in spec_lower and not any(t.isupper() for t in texts):
                    match_result = "‚ùå"
                    notes.append("Not All Caps")

                if "‚â•" in spec:
                    try:
                        m = re.search(r"‚â•\s*(\d+(?:\.\d+)?)", spec)
                        if m:
                            threshold = float(m.group(1))
                            if max_size < threshold:
                                match_result = "‚ùå"
                                notes.append(f"Font < {threshold} mm")
                            # ‡πÇ‡∏ä‡∏ß‡πå‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏à‡∏≠ (max) ‡∏Å‡∏±‡∏ô crash index 0
                            font_size_str = "‚úî" if match_result == "‚úî" else f"{round(max_size, 2)} mm"
                        else:
                            font_size_str = "-"
                    except Exception:
                        font_size_str = "-"
                else:
                    font_size_str = "‚úî" if match_result == "‚úî" else "-"
            else:
                font_size_str = "-"

            pages = sorted(set(found_pages))
            all_pages = sorted(set(p for _, p, _ in all_texts))
            page_str = "All pages" if set(pages) == set(all_pages) else ", ".join(str(p) for p in pages)

            if found_pages:
                logger.info("‚úÖ [FOUND] Req: '%s' | Term: '%s' | Pages: %s | Match: %s | Font: %s | Notes: %s",
                            requirement, term, (page_str or "-"),
                            match_result, font_size_str, (", ".join(notes) or "-"))
            else:
                logger.warning("‚ùå [NOT FOUND] Req: '%s' | Term tried: '%s'", requirement, term)
            
            grouped[(requirement, spec, "Verified")].append({
                "Term": term,
                "Found": found_flag,
                "Match": match_result if found_flag == "‚úÖ Found" else "‚ùå",
                "Pages": page_str if found_flag == "‚úÖ Found" else "-",
                "Font Size": font_size_str if found_flag == "‚úÖ Found" else "-",
                "Note": ", ".join(notes) if notes else "-",
                "Verification": "Verified",
                "Remark": remark_text,
                "Remark URL": remark_link,
                "Package Panel": package_panel,   
                "Procedure": procedure,           
            })

    final_results = []
    for (requirement, spec, verification), items in grouped.items():
        for item in items:
            term_display = item.get("Term", "")
            if pd.isna(term_display) or str(term_display).strip().lower() in ["", "nan"]:
                term_display = "-"
            final_results.append({
                "Requirement": requirement,
                "Symbol/ Exact wording": term_display,
                "Specification": spec,
                "Package Panel": item.get("Package Panel", "-"),
                "Procedure": item.get("Procedure", "-"),
                "Remark": item.get("Remark", "-"),
                "Remark URL": item.get("Remark URL", "-"), 
                "Found": item["Found"],
                "Match": item["Match"],
                "Pages": item["Pages"],
                "Font Size": item["Font Size"],
                "Note": item["Note"],
                "Verification": verification
            })

    return pd.DataFrame(final_results)